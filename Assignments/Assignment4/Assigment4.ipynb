{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f42ee522",
      "metadata": {
        "id": "f42ee522"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmika1234/dl_uwr/blob/develop/Assignments/Assignment4/Assignment4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ef34449",
      "metadata": {
        "id": "5ef34449"
      },
      "source": [
        "### Assigment 4\n",
        "\n",
        "**Submission deadlines**:\n",
        "\n",
        "* get at least 4 points by 17.05.2023\n",
        "* remaining points: last lab session before or on 26.05.2023\n",
        "\n",
        "**Points:** Aim to get 12 out of 15+ possible points\n",
        "\n",
        "All needed data files should be on Drive: <https://drive.google.com/drive/folders/1HaMbhzaBxxNa_z_QJXSDCbv5VddmhVVZ?usp=sharing> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "u3i4gtPTK31k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3i4gtPTK31k",
        "outputId": "3fe0a2de-491b-4034-f525-5fd4e246d624"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "# Please note that this code needs only to be run in a fresh runtime.\n",
        "# However, it can be rerun afterwards too.\n",
        "!pip install -q gdown httpimport\n",
        "# Huggingface Transformers implementation\n",
        "!pip install -q tqdm boto3 requests regex sentencepiece sacremoses\n",
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6d7ba628",
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "62805d42",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\dmika\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "C:\\Users\\dmika\\AppData\\Local\\Temp\\ipykernel_21820\\3274590660.py:18: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "import codecs\n",
        "\n",
        "import numpy as np\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "from scipy.spatial import distance\n",
        "\n",
        "import nltk\n",
        "import sklearn\n",
        "# nltk.download('punkt')\n",
        "\n",
        "import pandas as pd\n",
        "import pprint\n",
        "import gensim\n",
        "\n",
        "import io\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2ae3dc2",
      "metadata": {
        "id": "f2ae3dc2"
      },
      "source": [
        "## Task 1 (5 points)\n",
        "\n",
        "Implement simplified word2vec with negative sampling from scratch (using pure numpy). Assume that in the training data objects and contexts are given explicitly, one pair per line, and objects are on the left. The result of the training should be object vectors. Please, write them to a file using *natural* text format, ie\n",
        "\n",
        "<pre>\n",
        "word1 x1_1 x1_2 ... x1_N \n",
        "word2 x2_1 x2_2 ... x2_N\n",
        "...\n",
        "wordK xK_1 xK_2 ... xk_N\n",
        "</pre>\n",
        "\n",
        "Use the loss from Slide 25 in Lecture NLP.01, compute the gradient manually. You can use some gradient clipping, or regularisation. \n",
        "\n",
        "**Remark**: the data is specially prepared to make the learning process easier. \n",
        "Present vectors using the code below. In this task we define success as 'obtaining a result which looks definitely not random'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "xUoDkKdLOG0J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUoDkKdLOG0J",
        "outputId": "d347ca1b-ac14-4324-b22b-152bf8d61393"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1a6Rki9MJwxq8m6HdqgMoS_iSLIGXIlia&export=download\n",
            "To: /content/task1_objects_contexts_polish.txt.gz\n",
            "100% 47.0M/47.0M [00:00<00:00, 210MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1a6Rki9MJwxq8m6HdqgMoS_iSLIGXIlia&export=download\" -O DATA/task1_objects_contexts_polish.txt.gz\n",
        "!gzip -dkf DATA/task1_objects_contexts_polish.txt.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3f2b411b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# with open(\"DATA/task1_objects_contexts_polish.txt\", \"r\") as f:\n",
        "#     lines = f.readlines()\n",
        "# data = np.array([np.array(spair.strip().split()) for spair in lines])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "5a8601e9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5525116"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"DATA/task1_objects_contexts_polish.txt\", sep=\" \", header=None)\n",
        "data.columns = [\"word\", \"context\"]\n",
        "data.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "095a1f57",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.special import expit\n",
        "\n",
        "class Word2Vec:\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.word_embeddings = np.random.randn(vocab_size, embedding_dim)  # Initialize word embeddings randomly\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        vocab = np.sort(np.unique(pd.concat([df['word'], df['context']])))\n",
        "        word_to_id = dict(zip(vocab, np.arange(vocab.shape[0])))\n",
        "        id_to_word = dict(zip(word_to_id.values(), word_to_id.keys()))\n",
        "        words_ids = np.array(df['word'].map(word_to_id)).reshape(-1,1)\n",
        "        context_ids = np.array(df['context'].map(word_to_id)).reshape(-1,1)\n",
        "        data_pairs = np.concatenate((words_ids, context_ids), axis=1)\n",
        "        return word_to_id, id_to_word, data_pairs\n",
        "\n",
        "    def train(self, word_pairs, num_negative_samples, learning_rate, num_epochs):\n",
        "        for epoch in range(num_epochs):\n",
        "            loss_total = 0.0\n",
        "            np.random.shuffle(word_pairs)\n",
        "\n",
        "            for pair in word_pairs:\n",
        "                target_word, context_word = pair\n",
        "\n",
        "                # Generate negative samples\n",
        "                negative_samples = self.generate_negative_samples(num_negative_samples)\n",
        "\n",
        "                # Perform gradient updates for positive sample\n",
        "                loss_total += self.update(target_word, context_word, 1, learning_rate)\n",
        "\n",
        "                # Perform gradient updates for negative samples\n",
        "                for neg_sample in negative_samples:\n",
        "                    loss_total += self.update(target_word, neg_sample, 0, learning_rate)\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}/{num_epochs} - Average Loss: {loss_total / len(word_pairs)}\")\n",
        "\n",
        "    def update(self, target_word, context_word, label, learning_rate):\n",
        "        target_embedding = self.word_embeddings[target_word]\n",
        "        context_embedding = self.word_embeddings[context_word]\n",
        "\n",
        "        # Calculate score and predicted probability\n",
        "        score = np.dot(target_embedding, context_embedding)\n",
        "        predicted_prob = expit(score)\n",
        "\n",
        "        # Calculate the gradient of the loss with respect to the predicted probability\n",
        "        gradient = predicted_prob - label\n",
        "\n",
        "        # Update target word embedding\n",
        "        target_grad = gradient * context_embedding\n",
        "        self.word_embeddings[target_word] -= learning_rate * target_grad\n",
        "\n",
        "        # Update context word embedding\n",
        "        context_grad = gradient * target_embedding\n",
        "        self.word_embeddings[context_word] -= learning_rate * context_grad\n",
        "        epsilon = 1e-8\n",
        "        # Return the loss for monitoring\n",
        "        loss = -label * np.log(predicted_prob + epsilon) - (1 - label) * np.log(1 - predicted_prob + epsilon)\n",
        "        return loss\n",
        "\n",
        "    def generate_negative_samples(self, num_samples):\n",
        "        return np.random.randint(0, self.vocab_size, size=num_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "eb1601f9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - Average Loss: 12.650065629059531\n",
            "Epoch 2/10 - Average Loss: 7.427973104966454\n",
            "Epoch 3/10 - Average Loss: 6.202845315297416\n",
            "Epoch 4/10 - Average Loss: 5.366754035973531\n",
            "Epoch 5/10 - Average Loss: 4.5629757754196465\n",
            "Epoch 6/10 - Average Loss: 4.012112713928969\n",
            "Epoch 7/10 - Average Loss: 3.717774211000169\n",
            "Epoch 8/10 - Average Loss: 3.5384176510635057\n",
            "Epoch 9/10 - Average Loss: 3.4110770982675778\n",
            "Epoch 10/10 - Average Loss: 3.314003331146043\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(data['word'].unique()) + len(data['context'].unique())\n",
        "embedding_dim = 100  # Set the desired dimensionality of word embeddings\n",
        "num_negative_samples = 5  # Number of negative samples per positive sample\n",
        "learning_rate = 0.01  # Learning rate for gradient updates\n",
        "num_epochs = 10  # Number of training epochs\n",
        "\n",
        "# Create and preprocess the data\n",
        "model = Word2Vec(vocab_size, embedding_dim)\n",
        "word_to_id, id_to_word, data_pairs = model.preprocess_data(data)\n",
        "\n",
        "# Create and train the Word2Vec model\n",
        "model.train(data_pairs, num_negative_samples, learning_rate, num_epochs)\n",
        "\n",
        "# Access the learned word embeddings\n",
        "word_embeddings = model.word_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "24c232b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"DATA/word_embeddings.npy\", 'wb') as f:\n",
        "    np.save(f, word_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ae909d9",
      "metadata": {
        "id": "7ae909d9"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "task1_wv = KeyedVectors.load_word2vec_format('task1_w2v_vectors.txt', binary=False)\n",
        "\n",
        "example_english_words = ['dog', 'dragon', 'love', 'bicycle', 'marathon', 'logic', 'butterfly']  # replace, or add your own examples\n",
        "example_polish_words = ['pies', 'smok', 'miłość', 'rower', 'maraton', 'logika', 'motyl']\n",
        "\n",
        "example_words = example_polish_words\n",
        "\n",
        "for w0 in example_words:\n",
        "    print ('WORD:', w)\n",
        "    for w, v in task1_wv.most_similar(w0):\n",
        "        print ('   ', w, v)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41228961",
      "metadata": {
        "id": "41228961"
      },
      "source": [
        "## Task 2 (4 points)\n",
        "\n",
        "Your task is to train the embeddings for Simple Wikipedia titles, using gensim library. As the example below shows, training is really simple:\n",
        "\n",
        "```python\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(sentences=common_texts, vector_size=100, window=5, min_count=1, workers=4)\n",
        "model.save(\"word2vec.model\")\n",
        "```\n",
        "*sentences* can be a list of list of tokens, you can also use *gensim.models.word2vec.LineSentence(source)* to create restartable iterator from file. At first, use [this file] containing such pairs of titles, that one article links to another.\n",
        "\n",
        "We say that two titles are *related* if they both contain a word (or a word bigram) which is not very popular (it occurs only in several titles). Make this definition more precise, and create the corpora which contains pairs of related titles. Make a mixture of the original corpora, and the new one, then train title vectors again.\n",
        "\n",
        "Compare these two approaches using similar code to the code from Task 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f6ad8e4",
      "metadata": {
        "id": "2f6ad8e4"
      },
      "outputs": [],
      "source": [
        "# The cell for your presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d711fdca",
      "metadata": {
        "id": "d711fdca"
      },
      "source": [
        "# Task 3 (4 points)\n",
        "\n",
        "Suppose that we have two languages: Upper and Lower. This is an example Upper sentence:\n",
        "\n",
        "<pre>\n",
        "THE QUICK BROWN FOX JUMPS OVER THE LAZY DOG.\n",
        "</pre>\n",
        "\n",
        "And this is its translation into Lower:\n",
        "\n",
        "<pre>\n",
        "the quick brown fox jumps over the lazy dog\n",
        "</pre>\n",
        "\n",
        "You have two corpora for these languages (with different sentences). Your task is to train word embedings for both languages together, so as to make embeddings of the words which are its translations as close as possible. But unfortunately, you have the budget which allows you to prepare the translation only for 1000 words (we call it D, you have to deside which words you want to be in D)\n",
        "\n",
        "Prepare the corpora wich contains three kind of sentences:\n",
        "* Upper corpus sentences\n",
        "* Lower corpus sentences\n",
        "* sentences derived from Upper/Lower corpus, modified using D\n",
        "\n",
        "There are many possible ways of doing this, for instance this one (ROT13.COM: hfr rirel fragrapr sebz obgu pbecben gjvpr: jvgubhg nal zbqvsvpngvbaf, naq jvgu rirel jbeqf sebz Q ercynprq ol vgf genafyngvba)\n",
        "\n",
        "We define the score for an Upper WORD as  $\\frac{1}{p}$, where $p$ is a position of its translation in the list of **Lower** words most similar to WORD. For instance, when most similar words to DOG are:\n",
        "\n",
        "<pre>\n",
        "WOLF, CAT, WOLVES, LION, gopher, dog\n",
        "</pre>\n",
        "\n",
        "then the score for the word DOG is 0.5. Compute the average score separately for words from D, and for words out of D (hint: if the computation takes to much time do it for a random sample).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4947e307",
      "metadata": {
        "id": "4947e307"
      },
      "source": [
        "# Task 4 (4 points)\n",
        "\n",
        "In this task you are asked to do two things:\n",
        "1. compare the embeddings computed on small corpus (like Brown Corpus , see: <https://en.wikipedia.org/wiki/Brown_Corpus>) with the ones coming from Google News Corpus\n",
        "2. Try to use other resourses like WordNet to enrich to corpus, and obtain better embeddings\n",
        "\n",
        "You can use the following code snippets:\n",
        "\n",
        "```python\n",
        "# printing tokenized Brown Corpora\n",
        "from nltk.corpus import brown\n",
        "for s in brown.sents():\n",
        "    print(*s)\n",
        "    \n",
        "#iterating over all synsets in WordNet\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "for synset_type in 'avrns': # n == noun, v == verb, ...\n",
        "    for synset in list(wn.all_synsets(synset_type)))[:10]:\n",
        "        print (synset.definition())\n",
        "        print (synset.examples())\n",
        "        print ([lem.name() for lem in synset.lemmas()])\n",
        "        print (synset.hyperonims()) # nodes 1 level up in ontology\n",
        "        \n",
        "# loading model and compute cosine similarity between words\n",
        "\n",
        "model = Word2Vec.load('models/w2v.wordnet5.model') \n",
        "print (model.wv.similarity('dog', 'cat'))\n",
        "```\n",
        "\n",
        "Embeddings will be tested using WordSim-353 dataset, the code showing the quality is in the cell below. Prepare the following corpora:\n",
        "1. Tokenized Brown Corpora\n",
        "2. Definitions and examples from Princeton WordNet\n",
        "3. (1) and (2) together\n",
        "4. (3) enriched with pseudosentences containing (a subset) of WordNet knowledge (such as 'tiger is a carnivore')\n",
        "\n",
        "Train 4 Word2Vec models, and raport Spearman correletion between similarities based on your vectors, and similarities based on human judgements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "947a2fbc",
      "metadata": {
        "id": "947a2fbc"
      },
      "outputs": [],
      "source": [
        "# Code for computing correlation between W2V similarity, and human judgements\n",
        "\n",
        "import gensim.downloader\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "gn = gensim.downloader.load('word2vec-google-news-300')\n",
        "\n",
        "for similarity_type in ['relatedness', 'similarity']:\n",
        "    ws353 = []\n",
        "    for x in open(f'wordsim_{similarity_type}_goldstandard.txt'): \n",
        "        a,b,val = x.split()\n",
        "        val = float(val)\n",
        "        ws353.append( (a,b,val))\n",
        "    # spearmanr returns 2 vallues: correlation and pval. pval should be close to zero\n",
        "    print (similarity_type + ':', spearmanr(vals, ys)) "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "802bd33d",
      "metadata": {
        "id": "802bd33d"
      },
      "source": [
        "# Task 5 (4 points)\n",
        "\n",
        "Do the Problem 1 from old version of [Assigment 4](https://github.com/rnoxy/dl_uwr/blob/summer2023/Assignments/Assignment4.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7ee3d350",
      "metadata": {},
      "source": [
        "Many natural language processing tasks requrie continuous representations for words.\n",
        "[Word embeddings](https://en.wikipedia.org/wiki/Word_embedding) are mappings from a discrete\n",
        "space to real-valued vectors. Word embeddings might be trained with neural networks,\n",
        "either as a by-product of other tasks (e.g., language modeling, neural machine translation),\n",
        "or with networks designed specifically for the word embedding task.\n",
        "\n",
        "Two problems associated with training neural word embeddings are related to the speed of training:\n",
        "(a) large volume of data, on which the network has to be trained on, and (b) time required to compute\n",
        "output probability distribution over large vocabularities.\n",
        "\n",
        "One of the most popular architectures for training word embeddings is called Word2vec [[1]()], [[2]()]. It builds on the idea that semantics of a word can be defined through the contexts,\n",
        "in which the word appears in the sentence.\n",
        "\n",
        "Let $w_1, w_2,\\ldots,w_N$ be an $N$-word sentence in a natural language.\n",
        "We define a context of a word $w_l$ a list of $n$ preceeding and following words\n",
        "$w_{l-n},\\ldots,w_{l-1},w_{l+1},\\dots,w_{l+n}$.\n",
        "\n",
        "The underlying assumption is that similar words appear in similar contexts.\n",
        "For instance, words *Poland* and *Monaco* are similar in a sense, that they are singular nouns\n",
        "describing abstract concepts of existing, european countries.\n",
        "We can convince ourselves by looking at exceprts from Wikipedia articles\n",
        "on Poland and Monaco:\n",
        "\n",
        "* Despite **Monaco's independence** and separate foreign policy\n",
        "* aimed to preserve **Poland's independence** and the szlachta's\n",
        "\n",
        "* **Monaco joined the** Council of Europe in 2004\n",
        "* **Poland joined the** Schengen Area in 2007\n",
        "\n",
        "* nearly one-fifth **of Poland's population** – half of them\n",
        "* Christians comprise a total of 83.2% **of Monaco's population**.\n",
        "\n",
        "### Tasks\n",
        "You will use word vectors pre-computed on a large dataset.\n",
        "1. **[1p]** It has been observed, that word embeddings allow to perform semantic arithmetic where, for instance\n",
        "\n",
        "    **king** - **man** + **woman** ~= **queen**\n",
        "\n",
        "    This *analogy* task is often used as a quality measure of word embeddings. Load word embeddings and compute\n",
        "    their analogy score on a dataset of analogous pairs, expressed as an accuracy of accuracy of predicting a pair\n",
        "    item (**queen** in the example above). Specifically, compare `FastText` and `Word2vec` word embeddings.\n",
        "    \n",
        "2. **[1p]** Word embedding capture approximate semantics. Under an assumption that words of similar semantics\n",
        "    exist in different languages, a mapping $W: \\mathbb{R}^{300}\\mapsto\\mathbb{R}^{300}$ might be constructed that\n",
        "    translates word embeddings between languages. It has been shown that such ortonormal mappings allow to express\n",
        "    approximate, bilingual dictionaries. In addition, non-linear mappings do not offer additional benefits.\n",
        "\n",
        "    Given a simple English-Polish dictionary of word pairs (sourced from Wikitionary)\n",
        "    find an orthonormal mapping $W$ between English and Polish `FastText`\n",
        "    word embeddings using Procrustes analysis.\n",
        "\n",
        "3. **[1p]** Word embeddings can often be nicely visualized.\n",
        "    Make a 2-D `PCA` plot of word embeddings for countries and their capital cities\n",
        "    for `FastText` or `Word2vec`. Connect each country with its capital city with a line segment.\n",
        "    Can you see any regularities?\n",
        "    \n",
        "4. **[1p]** Plot 400 roughly most frequent words' embeddings (either `FastText` or `Word2vec`) in 2-D with `PCA`.\n",
        "    Skip stop words, punctuations, artifact words, etc. You can be imprecise and use heuristics\n",
        "    (e.g., select words than are at lest 3 charactes long).\n",
        "    Can you see any regularities? Another method of making meaningful visualizations is `t-SNE`.\n",
        "    \n",
        "    Make another 2-D visualization, this time using `t-SNE`. Visualizations with `t-SNE` are obtained\n",
        "    with gradient descent. Try to tweak optimization parameters to get lower optimization error,\n",
        "    than the one with default parameters.\n",
        "    Can you see any regularities this time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d7f0e5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download word vectors\n",
        "![ -e word2vec.tar.xz ] || gdown 'https://drive.google.com/uc?id=1v6D8IjYVFlonhQuN_J3PML5KSVQSpbED' -O word2vec.tar.xz\n",
        "![ -d word2vec ] || tar Jxf word2vec.tar.xz\n",
        "\n",
        "# Download conversation corpuses\n",
        "# ![ -e  hackernews_pairs.txt ] || gdown 'https://drive.google.com/uc?id=10cp2maNp1suzc5BaFQwDJr2GTKXHQOz_' -O hackernews_pairs.txt\n",
        "# ![ -e  reddit_pairs.txt ] || gdown 'https://drive.google.com/uc?id=1Uf0Xl9aqQVBBpOwhYTV7iWCwj95FDqtL' -O reddit_pairs.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7dc1cef5",
      "metadata": {},
      "source": [
        "## 5.1: Analogies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cfe3ac64",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Word embeddings assign a vector to each word. To use them we need three things:\n",
        "# 1. the embeddings themselves\n",
        "# 2. a dictionary from words to their embedding ids\n",
        "# 3. an inverse dictionary\n",
        "\n",
        "Embedding = collections.namedtuple(\n",
        "    'Embedding',\n",
        "    ['vec', 'word2idx', 'idx2word'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "4573b9c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_vecs_npy(base_path):\n",
        "    \"\"\"Load small embeddings in .npy format.\"\"\"\n",
        "    vec = np.load(base_path + '.npy')\n",
        "    idx2word = [l.strip() for l in codecs.open(\n",
        "                      base_path + '.txt', 'r', 'utf-8')]\n",
        "    word2idx = {w:i for (i,w) in enumerate(idx2word)}\n",
        "    return Embedding(vec, word2idx, idx2word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "bd7b6bb0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load top 200k word embeddings: Word2vec and FastText\n",
        "word2vec = load_vecs_npy('word2vec/word2vec_GoogleNews_200k')\n",
        "ftext = load_vecs_npy('word2vec/fasttext_wikien_200k')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "659b2177",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_analogies():\n",
        "    '''Load tuples of analogies, e.g., (man, woman, king, queen)'''\n",
        "    questions_path = 'word2vec/questions-words.txt'\n",
        "    analogies = [l.strip().split() for l in open(questions_path, 'r') \\\n",
        "                 if not l.startswith(':')]\n",
        "    return analogies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f469800",
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_analogies(vecs, analogies):\n",
        "    \"\"\"\n",
        "    Compute the accuracy on the analogy task.\n",
        "    \n",
        "    In the task, quadruples of words are given (q1, q2, r1, r2).\n",
        "    The words q1 and q2 are bound by a relation. The words r1 and r2 \n",
        "    are bound by the same relation. The task is to predict r2 knowing words\n",
        "    q1, q2, and r1.\n",
        "    \n",
        "    Example:\n",
        "    Quadruple [King, Queen, Man, Woman] yields the question\n",
        "    King is to Queen as Man is to ????\n",
        "    \n",
        "    Args:\n",
        "        word_vecs: Embeddings tupes\n",
        "        analogies: list of quadruples: (q1, q2, r1, r2)\n",
        "        \n",
        "    Returns:\n",
        "        percentage of correct answers\n",
        "    \"\"\"\n",
        "    indexes = [[vecs.word2idx.get(w, None) for w in tupl] \\\n",
        "                for tupl in analogies]\n",
        "    indexes = [tupl for tupl in indexes \\\n",
        "               if all([v is not None for v in tupl])]\n",
        "    # indexes holds quadruples of ints giving the ids of words from our vocab.\n",
        "    indexes = np.asarray(indexes)\n",
        "    print('Got vocab for %d/%d pairs' % (indexes.shape[0], len(analogies)))\n",
        "    \n",
        "    # Extract the vectors for the query words\n",
        "    q1 = vecs.vec[indexes[:, 0]]\n",
        "    q2 = vecs.vec[indexes[:, 1]]\n",
        "    r1 = vecs.vec[indexes[:, 2]]\n",
        "    \n",
        "    # Extractr the word IDs for the correct answers\n",
        "    r2_inds = indexes[:, 3]\n",
        "\n",
        "    #\n",
        "    # TODO \n",
        "    #\n",
        "    # Compute the approximate location of word r2 as r2 = r1 + (q2 - q1)\n",
        "    # Find the word closest to this location using cosine distance.\n",
        "    # Return it's id and compute the accurracy.\n",
        "    #\n",
        "\n",
        "    r2_pred = TODO\n",
        "    \n",
        "    \n",
        "    # Normalize length and compute dot product between r2_pred and word_vecs\n",
        "    # to get cosine distance\n",
        "    r2_pred_norm = TODO\n",
        "    vecs_norm = TODO\n",
        "    \n",
        "    # Compute in chunks to save memory\n",
        "    r2_pred_inds = np.concatenate([np.argmax(r2_pred_norm[i:i+1000].dot(vecs_norm.T), axis=1) \\\n",
        "                                   for i in range(0, r2_pred.shape[0], 1000)])\n",
        "    return 100.0 * (r2_pred_inds == r2_inds).sum() / r2_inds.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dc092be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load analogy tuples, e.g., (man, woman, king, queen)\n",
        "analogies = load_analogies()\n",
        "\n",
        "# Some are uppercased geographical names (and FastTexts are lowercased)\n",
        "analogies_lower = [[w.lower() for w in tupl] for tupl in analogies]\n",
        "\n",
        "print(analogies[0])\n",
        "print(analogies_lower[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff0fe905",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Keep in mind that the vocab is restricted to 200k most freq words\n",
        "# (in the training corpus!)\n",
        "print('FastText analogy task accuracy:')\n",
        "print('-------------------------------')\n",
        "# Fast-text was trained on lowercased text only\n",
        "print(eval_analogies(ftext, analogies_lower), '% correct')\n",
        "\n",
        "print('\\nWord2vec analogy task accuracy:')\n",
        "print('-------------------------------')\n",
        "# Word2vec has case information\n",
        "print(eval_analogies(word2vec, analogies), '% correct')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d97816b2",
      "metadata": {},
      "source": [
        "## 5.2: translation through alignment of vector spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cd14b53",
      "metadata": {},
      "outputs": [],
      "source": [
        "# We're need Polish embeddings\n",
        "ftext_pl = load_vecs_npy('word2vec/fasttext_wikipl_200k')\n",
        "\n",
        "# Load a simple wikitionary-based dict of word pairs\n",
        "en_pl = [l.strip().split('\\t') for l in codecs.open(\n",
        "    'word2vec/word2vec_en_pl', 'r', 'utf-8') if not '<UNK>' in l]\n",
        "en_pl = {t[0]:t[1] for t in en_pl if len(t) == 2}\n",
        "\n",
        "# Keep those, for which we have embeddings\n",
        "en_pl = {en:pl for (en,pl) in en_pl.items() \\\n",
        "         if en in ftext.word2idx and pl in ftext_pl.word2idx}\n",
        "print('Dictionary size:', len(en_pl))\n",
        "print('good --', en_pl['good'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92a846c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select word embeddings for dictionary words\n",
        "en_words = sorted(en_pl.keys())\n",
        "V_en = ftext.vec[[ftext.word2idx[w] for w in en_words]]\n",
        "V_pl = ftext_pl.vec[[ftext_pl.word2idx[en_pl[w]] for w in en_words]]\n",
        "print(V_en.shape, V_pl.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8a5f026",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find an orthogonal transformation from V_en to V_pl.\n",
        "# which minimizes square reconstruction error\n",
        "W = orthogonal_procrustes(V_en, V_pl)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8cf6e60",
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate(W, v, vecs2):\n",
        "    #\n",
        "    # TODO\n",
        "    #\n",
        "    # Transform source word vector v using W getting a location in target space.\n",
        "    # Find the closest word in target space using the cosine distance.\n",
        "    #\n",
        "    \n",
        "    return vecs2.idx2word[idx]\n",
        "\n",
        "# Select random, fairly frequent words not from the dictionary\n",
        "tr_words = [i for i in np.random.randint(1000, 2000, 100) \\\n",
        "            if ftext.idx2word[i] not in en_pl]\n",
        "tr_words = tr_words[:20]\n",
        "\n",
        "rows = [[ftext.idx2word[i], translate(W, ftext.vec[i], ftext_pl)] \\\n",
        "         for i in tr_words] \n",
        "print(tabulate.tabulate(rows))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7a5f3a5e",
      "metadata": {},
      "source": [
        "## 5.3: PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "078d8bf7",
      "metadata": {},
      "outputs": [],
      "source": [
        "capitals = [l.strip().split('\\t') for l in codecs.open('word2vec/countries_capitals', 'r', 'utf-8')]\n",
        "capitals = {country:capital for (country,capital) in capitals}\n",
        "\n",
        "# Select those present in Word2vec vocab\n",
        "capitals = {k:v for (k,v) in capitals.items() \\\n",
        "            if k in word2vec.word2idx and v in word2vec.word2idx}\n",
        "\n",
        "# Flatten the array to have a list of [country, capital, country, capital, ...]\n",
        "geo = [e for pair in capitals.items() for e in pair]\n",
        "print(len(geo) // 2, 'pairs', geo[:6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b475dca",
      "metadata": {},
      "outputs": [],
      "source": [
        "geo_inds = [word2vec.word2idx[w] for w in geo]\n",
        "geo_vecs = word2vec.vec[geo_inds]\n",
        "print(geo_vecs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2e7dd35",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#\n",
        "# TODO\n",
        "# use PCA from sklearn.decomposition to project the countries and capitals into 2D.\n",
        "# draw lines connecting each country with its capital\n",
        "#\n",
        "# Hint:\n",
        "# - the function \"annotate\" can be used to put text onto the plot\n",
        "#\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "dc2c7ae7",
      "metadata": {},
      "source": [
        "## 5.4: PCA vs t-SNE on frequent words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f100f83",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select words starting from the 200th to ommit stop words,\n",
        "# which have at least 3 chars\n",
        "top_words = [w for (i,w) in enumerate(ftext.idx2word) \\\n",
        "             if i > 200 and len(w) >= 3][:400]\n",
        "top_inds = [ftext.word2idx[w] for w in top_words]\n",
        "\n",
        "\n",
        "#\n",
        "# TODO: make a 2D PCA projection of the selected words.\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6be12abe",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "#\n",
        "# TODO: make a 2D t-SNE projection of the selected words.\n",
        "# Things will cluster much nicer\n",
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "685dd121",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: zomm in on 3 meaningful clusters\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
