{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3c880c8a",
      "metadata": {
        "id": "3c880c8a"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmika1234/dl_uwr/blob/develop/Assignments/Assignment5/assignment5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assigment 5\n",
        "\n",
        "**Submission deadlines**:\n",
        "\n",
        "* last lab before 20.06.2023\n",
        "\n",
        "**Points:** Aim to get 6 (updated value) out of 15+ possible points\n",
        "\n",
        "All needed data files are on Drive: <https://drive.google.com/drive/folders/1uufpGn46Mwv4oBwajIeOj4rvAK96iaS-?usp=sharing> (or will be soon :) )\n"
      ],
      "metadata": {
        "id": "mwSPd9P6qeZ3"
      },
      "id": "mwSPd9P6qeZ3"
    },
    {
      "cell_type": "markdown",
      "id": "faca5110",
      "metadata": {
        "id": "faca5110"
      },
      "source": [
        "## Task 1 (5 points)\n",
        "\n",
        "Consider the vowel reconstruction task -- i.e. inserting missing vowels (aeuioy) to obtain proper English text. For instance for the input sentence:\n",
        "\n",
        "<pre>\n",
        "h m gd smbd hs stln ll m vwls\n",
        "</pre>\n",
        "\n",
        "the best result is\n",
        "\n",
        "<pre>\n",
        "oh my god somebody has stolen all my vowels\n",
        "</pre>\n",
        "\n",
        "In this task both dev and test data come from the two books about Winnie-the-Pooh. You have to train two RNN Language Models on *pooh-train.txt*. For the first model use the code below, for the second choose different hyperparameters (different dropout, smaller number of units or layers, or just do any modification you want).\n",
        "\n",
        "The code below is based on\n",
        "https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"https://drive.google.com/uc?id=1-k8e9OG7NOVk73Kkv4WpqNQKHrVVmVXa\" -O pooh_train.txt\n",
        "!gdown \"https://drive.google.com/uc?id=1ADNyasf6AEUsmz-163DWHw_rSldfnpta\" -O pooh_test.txt\n",
        "!gdown \"https://drive.google.com/uc?id=1POiC9I_BjZKBQe-7XkW5CW0z8_6inWtY\" -O pooh_words.txt"
      ],
      "metadata": {
        "id": "t01ELcztQPvc",
        "outputId": "6a2711cc-a6ae-44e4-91b3-402060b49b29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "t01ELcztQPvc",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-k8e9OG7NOVk73Kkv4WpqNQKHrVVmVXa\n",
            "To: /content/pooh_train.txt\n",
            "100% 255k/255k [00:00<00:00, 116MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ADNyasf6AEUsmz-163DWHw_rSldfnpta\n",
            "To: /content/pooh_test.txt\n",
            "100% 34.6k/34.6k [00:00<00:00, 83.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1POiC9I_BjZKBQe-7XkW5CW0z8_6inWtY\n",
            "To: /content/pooh_words.txt\n",
            "100% 20.4k/20.4k [00:00<00:00, 83.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code and first model training"
      ],
      "metadata": {
        "id": "Cs6Jgjnsqkpn"
      },
      "id": "Cs6Jgjnsqkpn"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "78d50e03",
      "metadata": {
        "id": "78d50e03"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "SEQUENCE_LENGTH = 15\n",
        "\n",
        "class PoohDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, sequence_length, device):\n",
        "        txt = open('pooh_train.txt').read()\n",
        "\n",
        "        self.words = txt.lower().split() # The text is already tokenized\n",
        "\n",
        "        self.uniq_words = self.get_uniq_words()\n",
        "\n",
        "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
        "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
        "\n",
        "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
        "        self.sequence_length = sequence_length\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "    def get_uniq_words(self):\n",
        "        word_counts = Counter(self.words)\n",
        "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.words_indexes) - self.sequence_length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (\n",
        "            torch.tensor(self.words_indexes[index:index+self.sequence_length], device=self.device),\n",
        "            torch.tensor(self.words_indexes[index+1:index+self.sequence_length+1], device=self.device)\n",
        "        )\n",
        "\n",
        "pooh_dataset = PoohDataset(SEQUENCE_LENGTH, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "2b22a87b",
      "metadata": {
        "id": "2b22a87b"
      },
      "outputs": [],
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, dataset, device):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm_size = 512\n",
        "        self.embedding_dim = 100\n",
        "        self.num_layers = 2\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        n_vocab = len(dataset.uniq_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))\n",
        "\n",
        "model = LSTMModel(pooh_dataset, device)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "074d5f7c",
      "metadata": {
        "id": "074d5f7c",
        "outputId": "fbf2790b-fc1e-477c-89d0-4855a717a89e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'batch': 113, 'loss': 5.560373306274414}\n",
            "{'epoch': 1, 'batch': 113, 'loss': 5.049388408660889}\n",
            "{'epoch': 2, 'batch': 113, 'loss': 4.688978672027588}\n",
            "{'epoch': 3, 'batch': 113, 'loss': 4.363079071044922}\n",
            "{'epoch': 4, 'batch': 113, 'loss': 4.132043838500977}\n",
            "{'epoch': 5, 'batch': 113, 'loss': 3.9728634357452393}\n",
            "{'epoch': 6, 'batch': 113, 'loss': 3.8696725368499756}\n",
            "{'epoch': 7, 'batch': 113, 'loss': 3.7312123775482178}\n",
            "{'epoch': 8, 'batch': 113, 'loss': 3.6177730560302734}\n",
            "{'epoch': 9, 'batch': 113, 'loss': 3.5054659843444824}\n",
            "{'epoch': 10, 'batch': 113, 'loss': 3.3890910148620605}\n",
            "{'epoch': 11, 'batch': 113, 'loss': 3.284022808074951}\n",
            "{'epoch': 12, 'batch': 113, 'loss': 3.182687997817993}\n",
            "{'epoch': 13, 'batch': 113, 'loss': 3.09560489654541}\n",
            "{'epoch': 14, 'batch': 113, 'loss': 3.0284974575042725}\n",
            "{'epoch': 15, 'batch': 113, 'loss': 2.9465301036834717}\n",
            "{'epoch': 16, 'batch': 113, 'loss': 2.8474338054656982}\n",
            "{'epoch': 17, 'batch': 113, 'loss': 2.7900633811950684}\n",
            "{'epoch': 18, 'batch': 113, 'loss': 2.734464168548584}\n",
            "{'epoch': 19, 'batch': 113, 'loss': 2.633188247680664}\n",
            "{'epoch': 20, 'batch': 113, 'loss': 2.5672216415405273}\n",
            "{'epoch': 21, 'batch': 113, 'loss': 2.542318105697632}\n",
            "{'epoch': 22, 'batch': 113, 'loss': 2.482225179672241}\n",
            "{'epoch': 23, 'batch': 113, 'loss': 2.3926029205322266}\n",
            "{'epoch': 24, 'batch': 113, 'loss': 2.266077756881714}\n",
            "{'epoch': 25, 'batch': 113, 'loss': 2.1695001125335693}\n",
            "{'epoch': 26, 'batch': 113, 'loss': 2.081423282623291}\n",
            "{'epoch': 27, 'batch': 113, 'loss': 2.000039577484131}\n",
            "{'epoch': 28, 'batch': 113, 'loss': 1.9487297534942627}\n",
            "{'epoch': 29, 'batch': 113, 'loss': 1.8531758785247803}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 512\n",
        "max_epochs = 30\n",
        "\n",
        "def train(dataset, model):\n",
        "    model.train()\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        state_h, state_c = model.init_state(SEQUENCE_LENGTH)\n",
        "\n",
        "        for batch, (x, y) in enumerate(dataloader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)\n",
        "\n",
        "            state_h = state_h.detach()\n",
        "            state_c = state_c.detach()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
        "\n",
        "train(pooh_dataset, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "id": "92ad0a59",
      "metadata": {
        "id": "92ad0a59",
        "outputId": "32407702-95ba-45d4-c57c-9e4763e7c129",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (embedding): Embedding(2548, 100)\n",
              "  (lstm): LSTM(100, 512, num_layers=2, dropout=0.2)\n",
              "  (fc): Linear(in_features=512, out_features=2548, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ],
      "source": [
        "# torch.save(model.state_dict(), 'pooh_2x512_30ep.model')\n",
        "# model = LSTMModel(pooh_dataset, device)\n",
        "# model.load_state_dict(torch.load(\"pooh_2x512_30ep.model\"))\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "38520dec",
      "metadata": {
        "id": "38520dec",
        "outputId": "3f9fb32f-52d8-40aa-c261-b1b781b44d86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "in the morning pooh certain he thought by a friend or two , which shows piglet ! '' `` i see , '' said pooh . chapter viii . and pooh and piglet bent into a arms and said `` oh ! '' and that he was talking about . `` it 's all\n",
            "\n",
            "in the morning piglet remember drifted tuesday 'help pen r cuckoo spotted land vests enjoyed numb skoos help-yourself dried interrupted shoulders misty suggested board ice fought help-yourself seized referring stepped affectionately ease practised mole sea strip curious rains damp noise-you-make-before- curious line practised supper fluttered blue-bells outland slept flashed desert regretful beautiful sparkle slippery\n",
            "\n",
            "in the morning christopher robin -- -- he called piglet again . `` that 's just come in a sort of idea , pooh ? '' piglet said `` yes , yes , and then he had never had , piglet 's house as north . '' `` well , '' said pooh , ``\n",
            "\n",
            "in the morning rabbit giving front trap can pooh said `` oh ! '' he added brushing . `` it 's easy . come on out and could it , and then for the second one of roo and honey and honey outside backson . '' and the nicest lick where he was ,\n",
            "\n",
            "in the morning owl respects wedged will paints members table-cloth affectionate breathe peaceful stiffening organized extract including habit oo-noise vests roosbreakfast whole cuckoo fish cry school received throat march fluffy misty woods pictures packet careless eats 'help mentioned eats problem contrary sweetly squeakily instance refined practised handkerchief gift waterfall dearie thickish crawled drowsy watched\n",
            "\n",
            "in the morning tigger sudden waterfall spot bumped wolery numbed poohanpiglet thank-you curious sponge come-come unlucky wire stout practised sponge article wonders yapping flashed captainish curious completely rhyme balancing worraworraworraworraworra outdoor temporary motherly breathed news slept outdoor respects wonders l-let waterfall practised west earth planted extract happening simple therefore less wiped unsettled enjoyed packet\n",
            "\n",
            "in the morning eeyore pool herself sighed curl became honeycomb curl 10 sprawled waterproof neck shawl road flashed smiled rhyme 8 roosbreakfast er sponge horrible respects firs rhododendron learn peaceful outdoor gale working extract upwards swallow goodbye rhyme smallest contradiction light contradiction necessary contrary numbed road taught heap brush help-yourself cuckoo 'gone halt swallow\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# The predict function is a text generator. You have to modify this code!\n",
        "\n",
        "def predict(dataset, model, text, next_words=15):\n",
        "    model.eval()\n",
        "\n",
        "    words = text.split()\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "    for i in range(0, next_words):\n",
        "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
        "        x = x.to(device)\n",
        "\n",
        "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
        "\n",
        "        last_word_logits = y_pred[0][-1]\n",
        "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().cpu().numpy()\n",
        "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
        "        words.append(dataset.index_to_word[word_index])\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "# DEMO\n",
        "speakers = ['pooh', 'piglet', 'christopher robin', 'rabbit', 'owl', 'tigger', 'eeyore']\n",
        "for s in speakers:\n",
        "    prompt = 'in the morning ' + s\n",
        "    for i in range(1):\n",
        "        print (predict(pooh_dataset, model, prompt, 50))\n",
        "    print ()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "ea1eb733",
      "metadata": {
        "id": "ea1eb733",
        "outputId": "f91dbfea-9417-4ad1-8e4b-5f8bb0ee371c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "799\n"
          ]
        }
      ],
      "source": [
        "# You can use the code if you want\n",
        "\n",
        "from collections import defaultdict as dd\n",
        "\n",
        "vowels = set(\"aoiuye'\")\n",
        "def devowelize(s):\n",
        "    rv = ''.join(a for a in s if a not in vowels)\n",
        "    if rv:\n",
        "        return rv\n",
        "    return '_' # Symbol for words without consonants\n",
        "\n",
        "pooh_words = set(open('pooh_words.txt').read().split())\n",
        "representation = dd(set)\n",
        "\n",
        "for w in pooh_words:\n",
        "    r = devowelize(w)\n",
        "    if w in pooh_dataset.word_to_index.keys():\n",
        "      representation[r].add(w)\n",
        "\n",
        "hard_words = set()\n",
        "for r, ws in representation.items():\n",
        "    if len(ws) > 1:\n",
        "        hard_words.update(ws)\n",
        "\n",
        "print (len(hard_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### solutions -> results"
      ],
      "metadata": {
        "id": "lff_Exr2qq1y"
      },
      "id": "lff_Exr2qq1y"
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_probability(model, dataset, sequence):\n",
        "    model.eval()\n",
        "\n",
        "    words = sequence.split()\n",
        "    state_h, state_c = model.init_state(len(words))\n",
        "\n",
        "    x = torch.tensor([[dataset.word_to_index[w] for w in words]], device=device)\n",
        "    y_pred, _ = model(x, (state_h, state_c))\n",
        "\n",
        "    # softmax to get probabilities\n",
        "    p = torch.nn.functional.softmax(y_pred[0], dim=1).detach().cpu().numpy()\n",
        "\n",
        "    # get log-probability of the sequence\n",
        "    log_p = np.log(p[range(len(words)), [dataset.word_to_index[w] for w in words]]).sum()\n",
        "\n",
        "    return log_p\n",
        "\n",
        "def beam_search(model, devowelized_sentence, beam_width, dataset=pooh_dataset):\n",
        "    # Initialize the list of candidate sequences\n",
        "    candidates = [('', 0.0)]  # note that we initialize with log-probability 0\n",
        "\n",
        "    # Process each devowelized word in the sentence\n",
        "    for word in devowelized_sentence.split():\n",
        "        if len(representation[word]) != 0:\n",
        "          new_candidates = []\n",
        "\n",
        "          # For each candidate sequence, extend it with each possible reconstructed word\n",
        "          for seq, seq_log_p in candidates:\n",
        "              for reconstructed_word in representation[word]:\n",
        "                  new_seq = seq + ' ' + reconstructed_word if seq else reconstructed_word\n",
        "                  # Calculate the log-probability of the new sequence\n",
        "                  new_seq_log_p = seq_log_p + sequence_probability(model, dataset, new_seq)\n",
        "                  new_candidates.append((new_seq, new_seq_log_p))\n",
        "\n",
        "          # Sort the new candidates by log-probability in descending order\n",
        "          new_candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "          # Keep only the top candidates with highest log-probabilities\n",
        "          candidates = new_candidates[:beam_width]\n",
        "        else:\n",
        "          new_candidates = []\n",
        "          for seq, log_p in candidates:\n",
        "            new_seq = seq + ''\n",
        "            new_candidates.append((new_seq, log_p))\n",
        "          # print(f\"{word} skipped!\")\n",
        "          candidates = new_candidates[:beam_width]\n",
        "    # Return the candidate sequence with the highest overall log-probability\n",
        "    return candidates[0][0]"
      ],
      "metadata": {
        "id": "PjVkSca7YH3s"
      },
      "id": "PjVkSca7YH3s",
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "273881ae",
      "metadata": {
        "id": "273881ae"
      },
      "source": [
        "You can assume that only words from pooh_words.txt can occur in the reconstructed text. For decoding you have two options (choose one, or implement both ang get **+1** bonus point)\n",
        "\n",
        "1. Sample reconstructed text several times (with quite a low temperature), choose the most likely result.\n",
        "2. Perform beam search.\n",
        "\n",
        "Of course in the sampling procedure you should consider only words matching the given consonants.\n",
        "\n",
        "Report accuracy of your methods (for both language models). The accuracy should be computed by the following function, it should be *greater than 0.25*.\n",
        "\n",
        "\n",
        "```python\n",
        "def accuracy(original_sequence, reconstructed_sequence):\n",
        "    sa = original_sequence\n",
        "    sb = reconstructed_sequence\n",
        "    score = len([1 for (a,b) in zip(sa, sb) if a == b])\n",
        "    return score / len(original_sequence)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(original_sequence, reconstructed_sequence):\n",
        "    sa = original_sequence\n",
        "    sb = reconstructed_sequence\n",
        "    score = len([1 for (a,b) in zip(sa, sb) if a == b])\n",
        "    return score / len(original_sequence)"
      ],
      "metadata": {
        "id": "mtKS22jPguB9"
      },
      "id": "mtKS22jPguB9",
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "devowelized_sentence = \"h m gd smbd hs stln ll m vwls\"\n",
        "beam_width = 4  # You can adjust the beam width\n",
        "result = beam_search(model, devowelized_sentence, beam_width)\n",
        "print(f\"Reconstructed sentence: {result}\")\n",
        "print(f\"First model accuracy: {accuracy(result.split(), 'oh my god somebody has stolen all my vowels'.split()):.3f}\")"
      ],
      "metadata": {
        "id": "EMj2H87kgun9",
        "outputId": "e765a039-5677-4877-c343-297a0ed02d92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EMj2H87kgun9",
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstructed sentence: he my good somebody his stolen all my\n",
            "First model accuracy: 0.625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second model"
      ],
      "metadata": {
        "id": "iCsrK8tjqRSV"
      },
      "id": "iCsrK8tjqRSV"
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel2(nn.Module):\n",
        "    def __init__(self, dataset, device):\n",
        "        super(LSTMModel2, self).__init__()\n",
        "        self.lstm_size = 256\n",
        "        self.embedding_dim = 100\n",
        "        self.num_layers = 2\n",
        "        self.device = device\n",
        "\n",
        "\n",
        "        n_vocab = len(dataset.uniq_words)\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=n_vocab,\n",
        "            embedding_dim=self.embedding_dim,\n",
        "        )\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=self.embedding_dim,\n",
        "            hidden_size=self.lstm_size,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=0.2,\n",
        "        )\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(self.lstm_size, self.lstm_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(self.lstm_size, n_vocab))\n",
        "\n",
        "\n",
        "    def forward(self, x, prev_state):\n",
        "        embed = self.embedding(x)\n",
        "        output, state = self.lstm(embed, prev_state)\n",
        "        logits = self.fc(output)\n",
        "        return logits, state\n",
        "\n",
        "    def init_state(self, sequence_length):\n",
        "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device),\n",
        "                torch.zeros(self.num_layers, sequence_length, self.lstm_size).to(self.device))\n",
        "\n",
        "model2 = LSTMModel2(pooh_dataset, device)\n",
        "model2.to(device)"
      ],
      "metadata": {
        "id": "J4FZVkYZm4Tb",
        "outputId": "408894cf-ae0b-4805-95c0-3309feac33bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "J4FZVkYZm4Tb",
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel2(\n",
              "  (embedding): Embedding(2548, 100)\n",
              "  (lstm): LSTM(100, 256, num_layers=2, dropout=0.2)\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=2548, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(pooh_dataset, model2)"
      ],
      "metadata": {
        "id": "HI9b0WnAo6Zm",
        "outputId": "407d9425-742b-43ff-ce75-04eb2b69b6a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HI9b0WnAo6Zm",
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'epoch': 0, 'batch': 113, 'loss': 5.370290756225586}\n",
            "{'epoch': 1, 'batch': 113, 'loss': 4.9626994132995605}\n",
            "{'epoch': 2, 'batch': 113, 'loss': 4.6660475730896}\n",
            "{'epoch': 3, 'batch': 113, 'loss': 4.377070903778076}\n",
            "{'epoch': 4, 'batch': 113, 'loss': 4.202948093414307}\n",
            "{'epoch': 5, 'batch': 113, 'loss': 4.038951396942139}\n",
            "{'epoch': 6, 'batch': 113, 'loss': 3.903787612915039}\n",
            "{'epoch': 7, 'batch': 113, 'loss': 3.763707160949707}\n",
            "{'epoch': 8, 'batch': 113, 'loss': 3.6513772010803223}\n",
            "{'epoch': 9, 'batch': 113, 'loss': 3.552811861038208}\n",
            "{'epoch': 10, 'batch': 113, 'loss': 3.433504819869995}\n",
            "{'epoch': 11, 'batch': 113, 'loss': 3.2995903491973877}\n",
            "{'epoch': 12, 'batch': 113, 'loss': 3.1876747608184814}\n",
            "{'epoch': 13, 'batch': 113, 'loss': 3.08644437789917}\n",
            "{'epoch': 14, 'batch': 113, 'loss': 3.011042356491089}\n",
            "{'epoch': 15, 'batch': 113, 'loss': 2.9016997814178467}\n",
            "{'epoch': 16, 'batch': 113, 'loss': 2.787229061126709}\n",
            "{'epoch': 17, 'batch': 113, 'loss': 2.7463219165802}\n",
            "{'epoch': 18, 'batch': 113, 'loss': 2.677298069000244}\n",
            "{'epoch': 19, 'batch': 113, 'loss': 2.6314868927001953}\n",
            "{'epoch': 20, 'batch': 113, 'loss': 2.4826557636260986}\n",
            "{'epoch': 21, 'batch': 113, 'loss': 2.37614369392395}\n",
            "{'epoch': 22, 'batch': 113, 'loss': 2.2615315914154053}\n",
            "{'epoch': 23, 'batch': 113, 'loss': 2.151026964187622}\n",
            "{'epoch': 24, 'batch': 113, 'loss': 2.0732321739196777}\n",
            "{'epoch': 25, 'batch': 113, 'loss': 2.0123183727264404}\n",
            "{'epoch': 26, 'batch': 113, 'loss': 1.943729281425476}\n",
            "{'epoch': 27, 'batch': 113, 'loss': 1.8530759811401367}\n",
            "{'epoch': 28, 'batch': 113, 'loss': 1.7542216777801514}\n",
            "{'epoch': 29, 'batch': 113, 'loss': 1.6701040267944336}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model2.state_dict(), '2_pooh_2x512_30ep.model')\n",
        "# model2 = LSTMModel2(pooh_dataset, device)\n",
        "# model2.load_state_dict(torch.load(\"pooh_2x512_30ep.model\"))\n",
        "# model2.eval()\n",
        "# model2.to(device)"
      ],
      "metadata": {
        "id": "XNpGUSaHoO3Y"
      },
      "id": "XNpGUSaHoO3Y",
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.to(device)\n",
        "devowelized_sentence = \"h m gd smbd hs stln ll m vwls\"\n",
        "beam_width = 4  # You can adjust the beam width\n",
        "result = beam_search(model2, devowelized_sentence, beam_width)\n",
        "print(f\"Reconstructed sentence: {result}\")\n",
        "print(f\"Second model accuracy: {accuracy(result.split(), 'oh my god somebody has stolen all my vowels'.split()):.3f}\")"
      ],
      "metadata": {
        "id": "BLgUeZy0qCG2",
        "outputId": "b37520f0-e13c-486e-887e-c420c60d0b83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "BLgUeZy0qCG2",
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reconstructed sentence: oh my good somebody has stolen 'll my\n",
            "Second model accuracy: 0.750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a158dfd",
      "metadata": {
        "id": "9a158dfd"
      },
      "source": [
        "## Task 2 (6 points)\n",
        "\n",
        "This task is about text generation. You have to:\n",
        "\n",
        "**A**. Create text corpora containing texts with similar vocabulary (for instance books from the same genre, or written by the same author). This corpora should have approximately 1M words. You can consider using the following sources: Project Gutenberg (https://www.gutenberg.org/), Wolne Lektury (https://wolnelektury.pl/), parts of BookCorpus, https://github.com/soskek/bookcorpus, but generally feel free. Texts could be in English, Polish or any other language you know.\n",
        "\n",
        "**B**. choose the tokenization procedure. It should have two stages:\n",
        "\n",
        "1. word tokenization (you can use nltk.tokenize.word_tokenize, tokenizer from spaCy, pytorch, keras, ...). Test your tokenizer on your corpora, and look at a set of tokens containing both letters and special characters. If some of them should be in your opinion treated as a sequence of tokens, then modify the tokenization procedure\n",
        "\n",
        "2. sub-word tokenization (you can either use the existing procedure, like wordpiece or sentencepiece, or create something by yourself). Here is a simple idea: take 8K most popular words (W), 1K most popular suffixes (S), and 1K most popular prefixes (P). Words in W are its own tokens. Word x outside W should be tokenized as 'p_ _s' where p is the longest prefix of x in P, and s is the longest prefix of W\n",
        "\n",
        "**C**. write text generation procedure. The procedure should fulfill the following requirements:\n",
        "\n",
        "1. it should use the RNN language model (trained on sub-word tokens)\n",
        "2. generated tokens should be presented as a text containing words (without extra spaces, or other extra characters, as begin-of-word introduced during tokenization)\n",
        "3. all words in a generated text should belond to the corpora (note that this is not guaranteed by LSTM)\n",
        "4. in generation Top-P sampling should be used (see NN-NLP.6, slide X)\n",
        "5. in generated texts every token 3-gram should be uniq\n",
        "6. *(optionally, +1 point)* all token bigrams in generated texts occur in the corpora\n",
        "\n",
        "Of course to fulfill these constraints you have to do rejection sampling, or beam search, or ... If you want to be more up-to-date you can also use transformer-like language model. In this case consider using nanoGPT (by A. Karpathy)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41e84582",
      "metadata": {
        "id": "41e84582"
      },
      "source": [
        "## Task 3 (4 or 6 p)\n",
        "\n",
        "In this task you have to create a network which looks at characters of the word and tries to guess whether the word is a noun, a verb, an adjective, and so on. To be more precise: the input is a word (without context), the output is a POS-tag (Part-of-Speech). Since some words are unambiguous, and we have no context, our network is supposed to return the set of possible tags.\n",
        "\n",
        "The data is taken from Universal Dependencies English corpus, and of course it contains errors, especially because not all possible tags occured in the data.\n",
        "\n",
        "Train a network (4p) or two networks (+2p) solving this task. Both networks should look at character n-grams occuring in the word. There are two options:\n",
        "\n",
        "* **Fixed size:** for instance take 2,3, and 4-character suffixes of the word, use them as  features (whith 1-hot encoding). You can also combine prefix and suffix features. Simple, useful trick: when looking at suffixes, add some '_' characters at the beginning of the word to guarantee that shorter words have suffixes of a desired length.\n",
        "\n",
        "* **Variable size:** take for instance 4-grams (or 4 grams and 3-grams), use Deep Averaging Network. Simple trick: add extra character at the beginning and at the end of the word, to add the information, that ngram occurs at special position ('ed' at the end has slightly different meaning that 'ed' in the middle)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ac85fcb",
      "metadata": {
        "id": "4ac85fcb"
      },
      "source": [
        "## Task 4 (5p)\n",
        "\n",
        "Apply seq2seq model (you can modify the code from this tutorial: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) to compute grapheme to phoneme conversion for English. Train the model on dev_cmu_dict.txt and test it on test_cmu_dict.txt. Report accuracy of your solution using two metrics:\n",
        "* exact match (how many words are perfectly converted to phonemes)\n",
        "* exact match without stress (how many words are perfectly converted to phonemes when we remove the information about stress)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2da193ae",
      "metadata": {
        "id": "2da193ae"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2d5e47",
      "metadata": {
        "id": "7c2d5e47"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c538fb76",
      "metadata": {
        "id": "c538fb76"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4feefe2",
      "metadata": {
        "id": "b4feefe2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}