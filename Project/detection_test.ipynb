{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System\n",
    "import os\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Standard libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Image utils\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import cv2\n",
    "from glob import glob\n",
    "\n",
    "# PyTorch \n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets\n",
    "from torch import nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset, Subset\n",
    "from torchvision.io import read_image\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torchvision.datasets.folder import default_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance('asd', str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = '/kaggle/input/german-traffic-sign-detection-benchmark-gtsdb'\n",
    "DATA_PATH = 'DATA/GTSDB/'\n",
    "# TRAIN_DATA_PATH = os.path.join(DATA_PATH, 'TestIJCNN2013/TestIJCNN2013Download')\n",
    "TRAIN_DATA_PATH = os.path.join(DATA_PATH, 'Train')\n",
    "TEST_DATA_PATH = os.path.join(DATA_PATH, 'TrainIJCNN2013/TrainIJCNN2013')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignsDatasetYOLO(Dataset):\n",
    "    def __init__(self, img_dir, annotations_file, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.annotations = pd.read_csv(annotations_file, sep=\";\", header=None,\n",
    "                                        names=[\"filename\", \"x1\", \"y1\", \"x2\", \"y2\", \"class\"])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        # fetch all the bounding boxes and labels for the current image\n",
    "        for _, row in self.annotations[self.annotations.filename == self.annotations.iloc[idx, 0]].iterrows():\n",
    "            boxes.append([row[\"x1\"], row[\"y1\"], row[\"x2\"], row[\"y2\"]])\n",
    "            labels.append(row[\"class\"])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # change the coordinates of bounding boxes to x_center, y_center, width, height and normalize\n",
    "        boxes[:, :2] = (boxes[:, :2] + boxes[:, 2:]) / 2.0  # x_center, y_center\n",
    "        boxes[:, 2:] = boxes[:, 2:] - boxes[:, :2]  # width, height\n",
    "        boxes /= torch.tensor([image.width, image.height, image.width, image.height])\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        \n",
    "        if self.transform:\n",
    "            image, target = self.transform(image, target)\n",
    "            \n",
    "        return image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = TrafficSignsDatasetYOLO(os.path.join(DATA_PATH, 'Train'), os.path.join(DATA_PATH, 'gt_jpg.txt'))\n",
    "loader = iter(dt)\n",
    "next(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignsDatasetRCNN(Dataset):\n",
    "    def __init__(self, img_dir, annotations_file, transforms=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transforms = transforms\n",
    "        self.annotations = pd.read_csv(annotations_file, sep=\";\", header=None,\n",
    "                                    names=[\"filename\", \"x1\", \"y1\", \"x2\", \"y2\", \"class\"])\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image path\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        #Load image as PIL\n",
    "        img = Image.open(img_path).convert(\"RGB\")        \n",
    "        # Get objects\n",
    "        objects = self.annotations[self.annotations.filename == self.annotations.iloc[idx, 0]]\n",
    "        # Get bounding box coordinates for each object in image\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        # fetch all the bounding boxes and labels for the current image\n",
    "        for _, row in objects.iterrows():\n",
    "            boxes.append([row[\"x1\"], row[\"y1\"], row[\"x2\"], row[\"y2\"]])\n",
    "            labels.append(row[\"class\"])\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)        \n",
    " \n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # suppose all instances are not crowd\n",
    "        iscrowd = torch.zeros((len(objects),), dtype=torch.int64)\n",
    " \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    " \n",
    "        if self.transforms is not None:\n",
    "            # Note that target (including bbox) is also transformed\\enhanced here, which is different from transforms from torchvision import\n",
    "            # Https://github.com/pytorch/vision/tree/master/references/detectionOfTransforms.pyThere are examples of target transformations when RandomHorizontalFlip\n",
    "            img, target = self.transforms(img, target)\n",
    " \n",
    "        return img, target\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = TrafficSignsDatasetRCNN(os.path.join(DATA_PATH, 'Train'), os.path.join(DATA_PATH, 'gt_jpg.txt'))\n",
    "loader = iter(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=1360x800>,\n",
       " {'boxes': tensor([[ 998.,  292., 1074.,  371.],\n",
       "          [  46.,  350.,  136.,  444.],\n",
       "          [ 945.,  435., 1006.,  492.]]),\n",
       "  'labels': tensor([ 1, 23, 23]),\n",
       "  'image_id': tensor([33]),\n",
       "  'area': tensor([6004., 8460., 3477.]),\n",
       "  'iscrowd': tensor([0, 0, 0])})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UniEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
